{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "be4nBharassa"
   },
   "source": [
    "The first step in our process was to scrape the news articles.  We decided to use News API, because it covered a large range of sources and was free to use for our purposes.  One drawback was that it only allowed us to scrape articles in the past 30 days.  \n",
    "\n",
    "We used the Media Bias chart created by Ad Fontes Media to determine which sources to pull from.  Our initial scrape we decided to use 1 conservative and 1 liberal source that were described as hyper partisan by the chart and 2 sources that were described as skewed.  We then scraped 3 sources that were described as neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vBZB8O_abuYb"
   },
   "source": [
    "News API requires you to use a keyword (denoted by 'q = ') to scrape articles, it will scrape the articles that have that keyword located within them.  For our initial scrape we decided to use broad terms to see how many articles we can get.  Further scrapes we may need to be more precise about the terms we use, but need to consider how that may bias our results.\n",
    "\n",
    "The pageSize attribute is how many articles from the batch that will be included in the list object that is retrieved.  For our initial scrape we decided to use 100.  In future scrapes we may decide to use a higher number, or lower.\n",
    "\n",
    "Sorting by popularity will allow us to retrieve the retrieve the most viewed articles of the bunch.  We may also need to consider how this could potentially bias our results.\n",
    "\n",
    "This process was repeated almost identically for the Conservative, Liberal, and Neutral news sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JPK106ZH832q"
   },
   "source": [
    "# Conservative Sources\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2WVE-maUmXq2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "url_fox1 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=primary&'\n",
    "       'sources=fox-news&'\n",
    "       'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_fox1 = requests.get(url_fox1)\n",
    "\n",
    "#print(response_fox1.content)\n",
    "\n",
    "url_fox2 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=virus&'\n",
    "       'sources=fox-news&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_fox2 = requests.get(url_fox2)\n",
    "\n",
    "#response_fox2.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QQOA-qUG6Zn2"
   },
   "outputs": [],
   "source": [
    "url_nreview1 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=primary&'\n",
    "       'sources=national-review&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_nreview1 = requests.get(url_nreview1)\n",
    "\n",
    "#print(response_nreview1.content)\n",
    "\n",
    "url_nreview2 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=virus&'\n",
    "       'sources=national-review&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_nreview2 = requests.get(url_nreview2)\n",
    "\n",
    "#print(response_nreview2.content)\n",
    "\n",
    "url_nreview3 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=what&'\n",
    "       'sources=national-review&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_nreview3 = requests.get(url_nreview3)\n",
    "\n",
    "#response_nreview3.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HGgmVNLT7Pk2"
   },
   "outputs": [],
   "source": [
    "url_bart1 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=virus&'\n",
    "       'sources=breitbart-news&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_bart1 = requests.get(url_bart1)\n",
    "\n",
    "#print(response_bart1.content)\n",
    "\n",
    "url_bart2 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=primary&'\n",
    "       'sources=breitbart-news&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_bart2 = requests.get(url_bart2)\n",
    "\n",
    "#response_bart2.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8TuNe--_8yea"
   },
   "source": [
    "# Liberal Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qV-NlflP8xAt"
   },
   "outputs": [],
   "source": [
    "url_vice1 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=virus&'\n",
    "       'sources=vice-news&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_vice1 = requests.get(url_vice1)\n",
    "\n",
    "#print(response_vice1.content)\n",
    "\n",
    "url_vice2 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=what&'\n",
    "       'sources=vice-news&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_vice2 = requests.get(url_vice2)\n",
    "\n",
    "#print(response_vice2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SobKN6njAU01"
   },
   "outputs": [],
   "source": [
    "url_msnbc1 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=virus&'\n",
    "       'sources=msnbc&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_msnbc1 = requests.get(url_msnbc1)\n",
    "\n",
    "#print(response_msnbc1.content)\n",
    "\n",
    "url_msnbc2 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=what&'\n",
    "       'sources=msnbc&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_msnbc2 = requests.get(url_msnbc2)\n",
    "\n",
    "#print(response_msnbc2.content)\n",
    "\n",
    "url_msnbc3 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=primary&'\n",
    "       'sources=msnbc&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_msnbc3 = requests.get(url_msnbc3)\n",
    "\n",
    "#print(response_msnbc3.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b-kC7Wi1BcZO"
   },
   "outputs": [],
   "source": [
    "url_buzz1 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=virus&'\n",
    "       'sources=buzzfeed&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_buzz1 = requests.get(url_buzz1)\n",
    "\n",
    "#print(response_buzz1.content)\n",
    "\n",
    "url_buzz2 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=primary&'\n",
    "       'sources=buzzfeed&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_buzz2 = requests.get(url_buzz2)\n",
    "\n",
    "#print(response_buzz2.content)\n",
    "\n",
    "url_buzz3 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=news&'\n",
    "       'sources=buzzfeed&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_buzz3 = requests.get(url_buzz3)\n",
    "\n",
    "#print(response_buzz3.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8l1twfXjCGPb"
   },
   "source": [
    "# Neutral Sources\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6A2Yz537CFSE"
   },
   "outputs": [],
   "source": [
    "url_cbs1 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=virus&'\n",
    "       'sources=cbs-news&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_cbs1 = requests.get(url_cbs1)\n",
    "\n",
    "#print(response_cbs1.content)\n",
    "\n",
    "url_cbs2 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=primary&'\n",
    "       'sources=cbs-news&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_cbs2 = requests.get(url_cbs2)\n",
    "\n",
    "#print(response_cbs2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TDtcr4HpLPgv"
   },
   "outputs": [],
   "source": [
    "url_hill1 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=primary&'\n",
    "       'sources=the-hill&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_hill1 = requests.get(url_hill1)\n",
    "\n",
    "#print(response_hill1.content)\n",
    "\n",
    "url_hill2 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=virus&'\n",
    "       'sources=the-hill&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_hill2 = requests.get(url_hill2)\n",
    "\n",
    "#print(response_hill2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3_lcHrwmLoGE"
   },
   "outputs": [],
   "source": [
    "url_reuters1 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=primary&'\n",
    "       'sources=reuters&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_reuters1 = requests.get(url_reuters1)\n",
    "\n",
    "#print(response_reuters1.content)\n",
    "\n",
    "url_reuters2 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=trump&'\n",
    "       'sources=reuters&'\n",
    "       'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-03-15&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_reuters2 = requests.get(url_reuters2)\n",
    "\n",
    "#print(response_reuters2.content)\n",
    "# response_reuters2.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z7T04boIMGx_"
   },
   "source": [
    "# Decoding and Converting to JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iOxXM5MheLUD"
   },
   "source": [
    "Using the decode function was necessary to translate the list objects from News API so they could be converted to a json format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "colab_type": "code",
    "id": "gd5YejjpvSXR",
    "outputId": "95a63522-0b24-4e91-d96b-680c5bccc0f7"
   },
   "outputs": [],
   "source": [
    "# response_fox1\n",
    "# response_fox2\n",
    "# response_bart1\n",
    "# response_bart2\n",
    "# response_nreview1\n",
    "# response_nreview2\n",
    "# response_nreview3\n",
    "\n",
    "print('Conservative sources, 1 partisan right, 2 skew right')\n",
    "response_fox1_d = response_fox1.content.decode(\"utf-8\")\n",
    "print(response_fox1_d)\n",
    "response_fox2_d = response_fox2.content.decode(\"utf-8\")\n",
    "print(response_fox2_d)\n",
    "\n",
    "response_bart1_d = response_bart1.content.decode(\"utf-8\")\n",
    "#print(response_bart1_d)\n",
    "response_bart2_d = response_bart2.content.decode(\"utf-8\")\n",
    "#print(response_bart2_d)\n",
    "\n",
    "response_nreview1_d = response_nreview1.content.decode(\"utf-8\")\n",
    "#print(response_nreview1_d)\n",
    "response_nreview2_d = response_nreview2.content.decode(\"utf-8\")\n",
    "#print(response_nreview2_d)\n",
    "response_nreview3_d = response_nreview3.content.decode(\"utf-8\")\n",
    "#print(response_nreview3_d)\n",
    "\n",
    "\n",
    "# response_vice1\n",
    "# response_vice2\n",
    "# response_msnbc1\n",
    "# response_msnbc2\n",
    "# response_msnbc3\n",
    "# response_buzz1\n",
    "# response_buzz2\n",
    "# response_buzz3\n",
    "\n",
    "print('Liberal sources, 1 partisan left, 2 skew left')\n",
    "response_vice1_d = response_vice1.content.decode(\"utf-8\")\n",
    "#print(response_vice1_d)\n",
    "response_vice2_d = response_vice2.content.decode(\"utf-8\")\n",
    "#print(response_vice2_d)\n",
    "\n",
    "response_msnbc1_d = response_msnbc1.content.decode(\"utf-8\")\n",
    "#print(response_msnbc1_d)\n",
    "response_msnbc2_d = response_msnbc2.content.decode(\"utf-8\")\n",
    "#print(response_msnbc2_d)\n",
    "response_msnbc3_d = response_msnbc3.content.decode(\"utf-8\")\n",
    "#print(response_msnbc3_d)\n",
    "\n",
    "response_buzz1_d = response_buzz1.content.decode(\"utf-8\")\n",
    "#print(response_buzz1_d)\n",
    "response_buzz2_d = response_buzz2.content.decode(\"utf-8\")\n",
    "#print(response_buzz2_d)\n",
    "response_buzz3_d = response_buzz3.content.decode(\"utf-8\")\n",
    "#print(response_buzz3_d)\n",
    "\n",
    "\n",
    "\n",
    "# response_hill1\n",
    "# response_hill2\n",
    "# response_reuters1\n",
    "# response_reuters2\n",
    "# response_cbs1\n",
    "# response_cbs2\n",
    "\n",
    "print('Neutral sources')\n",
    "response_hill1_d = response_hill1.content.decode(\"utf-8\")\n",
    "#print(response_hill1_d)\n",
    "response_hill2_d = response_hill2.content.decode(\"utf-8\")\n",
    "#print(response_hill2_d)\n",
    "\n",
    "response_reuters1_d = response_reuters1.content.decode(\"utf-8\")\n",
    "#print(response_reuters1_d)\n",
    "response_reuters2_d = response_reuters2.content.decode(\"utf-8\")\n",
    "#print(response_reuters2_d)\n",
    "\n",
    "response_cbs1_d = response_cbs1.content.decode(\"utf-8\")\n",
    "#print(response_cbs1_d)\n",
    "response_cbs2_d = response_cbs2.content.decode(\"utf-8\")\n",
    "#print(response_cbs2_d)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(type(response_cbs2))\n",
    "type(response_cbs2_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QTqEDn8XqMQs"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# response_1x = json.loads(response_1)\n",
    "# print(type(response_1x))\n",
    "# # print(response_1x)\n",
    "# response_1x['articles']\n",
    "\n",
    "\n",
    "\n",
    "#conservative\n",
    "response_fox1_j = json.loads(response_fox1_d)\n",
    "# response_fox1_j['articles']\n",
    "# response_fox1_j\n",
    "response_fox2_j = json.loads(response_fox2_d)\n",
    "\n",
    "\n",
    "response_bart1_j = json.loads(response_bart1_d)\n",
    "response_bart2_j = json.loads(response_bart2_d)\n",
    "\n",
    "response_nreview1_j = json.loads(response_nreview1_d)\n",
    "response_nreview2_j = json.loads(response_nreview2_d)\n",
    "response_nreview3_j = json.loads(response_nreview3_d)\n",
    "\n",
    "#liberal\n",
    "response_vice1_j = json.loads(response_vice1_d)\n",
    "response_vice2_j = json.loads(response_vice2_d)\n",
    "\n",
    "response_msnbc1_j = json.loads(response_msnbc1_d)\n",
    "response_msnbc2_j = json.loads(response_msnbc2_d)\n",
    "response_msnbc3_j = json.loads(response_msnbc3_d)\n",
    "\n",
    "response_buzz1_j = json.loads(response_buzz1_d)\n",
    "response_buzz2_j = json.loads(response_buzz2_d)\n",
    "response_buzz3_j = json.loads(response_buzz3_d)\n",
    "\n",
    "\n",
    "#neutral\n",
    "response_hill1_j = json.loads(response_hill1_d)\n",
    "response_hill2_j = json.loads(response_hill2_d)\n",
    "\n",
    "response_reuters1_j = json.loads(response_reuters1_d)\n",
    "response_reuters2_j = json.loads(response_reuters2_d)\n",
    "\n",
    "response_cbs1_j = json.loads(response_cbs1_d)\n",
    "response_cbs2_j = json.loads(response_cbs2_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rjT74B06hv_K"
   },
   "source": [
    "# Converting JSON to pandas Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ox0m72hmeeTn"
   },
   "source": [
    "We then used pandas to transform the each JSON object into a dataframe that could be easily read and wrangled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y_CbPYqQqMcE"
   },
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(response_1x['articles'])\n",
    "# df\n",
    "\n",
    "#conservative df's\n",
    "df_fox1 = pd.DataFrame(response_fox1_j['articles'])\n",
    "df_fox1\n",
    "df_fox2 = pd.DataFrame(response_fox2_j['articles'])\n",
    "df_fox2\n",
    "\n",
    "\n",
    "df_bart1 = pd.DataFrame(response_bart1_j['articles'])\n",
    "df_bart1\n",
    "df_bart2 = pd.DataFrame(response_bart2_j['articles'])\n",
    "df_bart2\n",
    " \n",
    "\n",
    "df_nreview1 = pd.DataFrame(response_nreview1_j['articles'])\n",
    "df_nreview1\n",
    "df_nreview2 = pd.DataFrame(response_nreview2_j['articles'])\n",
    "df_nreview2\n",
    "df_nreview3 = pd.DataFrame(response_nreview3_j['articles'])\n",
    "df_nreview3\n",
    "\n",
    "\n",
    "# #liberal\n",
    "\n",
    "df_vice1 = pd.DataFrame(response_vice1_j['articles'])\n",
    "df_vice1\n",
    "df_vice2 = pd.DataFrame(response_vice2_j['articles'])\n",
    "df_vice2\n",
    "\n",
    "df_msnbc1 = pd.DataFrame(response_msnbc1_j['articles'])\n",
    "df_msnbc1\n",
    "df_msnbc2 = pd.DataFrame(response_msnbc2_j['articles'])\n",
    "df_msnbc2\n",
    "df_msnbc3 = pd.DataFrame(response_msnbc3_j['articles'])\n",
    "df_msnbc3\n",
    "\n",
    "df_buzz1 = pd.DataFrame(response_buzz1_j['articles'])\n",
    "df_buzz1\n",
    "df_buzz2 = pd.DataFrame(response_buzz2_j['articles'])\n",
    "df_buzz2\n",
    "df_buzz3 = pd.DataFrame(response_buzz3_j['articles'])\n",
    "df_buzz3\n",
    "\n",
    "\n",
    "# #neutral\n",
    "df_hill1 = pd.DataFrame(response_hill1_j['articles'])\n",
    "df_hill1\n",
    "df_hill2 = pd.DataFrame(response_hill2_j['articles'])\n",
    "df_hill2\n",
    "\n",
    "df_reuters1 = pd.DataFrame(response_reuters1_j['articles'])\n",
    "df_reuters1\n",
    "df_reuters2 = pd.DataFrame(response_reuters2_j['articles'])\n",
    "df_reuters2\n",
    "\n",
    "df_cbs1 = pd.DataFrame(response_cbs1_j['articles'])\n",
    "df_cbs1\n",
    "df_cbs2 = pd.DataFrame(response_cbs1_j['articles'])\n",
    "df_cbs2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "47F6gNL1j3Pk"
   },
   "source": [
    "# Merging Dataframes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MXynSQdXe0gC"
   },
   "source": [
    "After converting each JSON object into it's own dataframe, the pandas concat function was used to attach all of the rows of each of these dataframes together, creating one large dataframe of all of the scraped articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zu_vrHCFkfSX"
   },
   "outputs": [],
   "source": [
    "df_allsources = pd.concat([df_fox1, df_fox2, df_bart1, df_bart2, df_nreview1, df_nreview2, df_nreview3,\n",
    "                           df_msnbc1, df_msnbc2, df_msnbc3, df_buzz1, df_buzz2, df_buzz3, df_vice1, df_vice2,\n",
    "                           df_hill1, df_hill2, df_reuters1, df_reuters2, df_cbs1, df_cbs2])\n",
    "df_allsources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zuJA91HYl8zT"
   },
   "outputs": [],
   "source": [
    "df_allsources.to_csv('newsapi_scrape1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8sEt8SbNaaj_"
   },
   "source": [
    "# Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6PaXPnO0aakA"
   },
   "source": [
    "#### We first had a conversation about which part of the API's we were going to be using. There were news articles that didn't have content, and if it did have content, it wasn't fully there. So going forward with cleaning we decided that we should gather all that we could as far as content wise, and we could decide what to do with it later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OE8lGfyXaakB",
    "outputId": "ead9ea47-84a7-4b8b-b7ca-85d00d32e2e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 453 news articles that do not have content\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "MarchNews = pd.read_csv('newsapi_scrape1.csv')\n",
    "#print(MarchNews)\n",
    "\n",
    "Nan = MarchNews['content'].isnull().sum()\n",
    "print(f'There are {Nan} news articles that do not have content')\n",
    "\n",
    "\n",
    "#Replacing Null values with 'None'\n",
    "MarchNews['content'].fillna(\"None\", inplace = True)\n",
    "MarchNews['title'].fillna('None', inplace = True)\n",
    "MarchNews['description'].fillna('None', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IbOwDRKsaakF"
   },
   "source": [
    "#### The code underneath is finding all of the distinct words in the content, description, and title columns. Also using the re.findall function we were also able to remove the punctuation from the columns as well. After combing through the data we realized that the last two words were for the number of characters after whatever we had, so we scraped the last two entries in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sfZpT6lraakG"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "\n",
    "Content = []\n",
    "Description = []\n",
    "Title = []\n",
    "\n",
    "for i in range(len(MarchNews['content'])):\n",
    "    Content.append(re.findall(r'\\w+', MarchNews['content'][i])[:-3])\n",
    "    Description.append(re.findall(r'\\w+', MarchNews['description'][i]))\n",
    "    Title.append(re.findall(r'\\w+', MarchNews['title'][i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dvl5q-P6aakK"
   },
   "source": [
    "#### The codeblock below is taking out any stopwords, and removing any numbers or unknown symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rmwQUPGqaakK"
   },
   "outputs": [],
   "source": [
    "# Getting rid of Stopwords\n",
    "\n",
    "for i in range(len(Content)):\n",
    "    # Taking out the Stopwords\n",
    "    Content[i] = [words.lower() for words in Content[i] if words.lower() not in stopwords]\n",
    "    Title[i] = [words.lower() for words in Title[i] if words.lower() not in stopwords]\n",
    "    Description[i] = [words.lower() for words in Description[i] if words.lower() not in stopwords]\n",
    "    \n",
    "    # Taking out the non-string values\n",
    "    Content[i] = [words for words in Content[i] if words.isalpha() == True]\n",
    "    Title[i] = [words for words in Title[i] if words.isalpha() == True]\n",
    "    Description[i] = [words for words in Description[i] if words.isalpha() == True]\n",
    "    \n",
    "#print(Title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RgpOdbTQaakN",
    "outputId": "b4d3f609-9413-470e-d99f-b28aaa498fb3"
   },
   "outputs": [],
   "source": [
    "MarchNews.insert(8,'TitleCleaned', Title)\n",
    "MarchNews.insert(9,'DescriptionCleaned', Description)\n",
    "MarchNews.insert(10,'ContentCleaned', Content)\n",
    "#print(MarchNews[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mwWQNk6maakP",
    "outputId": "c8174f9b-eaf0-407b-e849-2943907a362b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1784 entries, 0 to 1783\n",
      "Data columns (total 11 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   source              1784 non-null   object\n",
      " 1   author              1745 non-null   object\n",
      " 2   title               1784 non-null   object\n",
      " 3   description         1784 non-null   object\n",
      " 4   url                 1784 non-null   object\n",
      " 5   urlToImage          1751 non-null   object\n",
      " 6   publishedAt         1784 non-null   object\n",
      " 7   content             1784 non-null   object\n",
      " 8   TitleCleaned        1784 non-null   object\n",
      " 9   DescriptionCleaned  1784 non-null   object\n",
      " 10  ContentCleaned      1784 non-null   object\n",
      "dtypes: object(11)\n",
      "memory usage: 153.4+ KB\n"
     ]
    }
   ],
   "source": [
    "MarchNews.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get word count in matrix \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>urlToImage</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>content</th>\n",
       "      <th>TitleCleaned</th>\n",
       "      <th>DescriptionCleaned</th>\n",
       "      <th>ContentCleaned</th>\n",
       "      <th>uniqueWords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'id': 'fox-news', 'name': 'Fox News'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sanders supporters react to DNC reportedly att...</td>\n",
       "      <td>Fox News contributor Lawrence Jones speaks to ...</td>\n",
       "      <td>http://video.foxnews.com/v/6136724620001/</td>\n",
       "      <td>https://cf-images.us-east-1.prod.boltdns.net/v...</td>\n",
       "      <td>2020-02-28T03:43:40Z</td>\n",
       "      <td>None</td>\n",
       "      <td>[sanders, supporters, react, dnc, reportedly, ...</td>\n",
       "      <td>[fox, news, contributor, lawrence, jones, spea...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{give, leadership, sanders, lawrence, fox, nom...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   source author  \\\n",
       "1  {'id': 'fox-news', 'name': 'Fox News'}    NaN   \n",
       "\n",
       "                                               title  \\\n",
       "1  Sanders supporters react to DNC reportedly att...   \n",
       "\n",
       "                                         description  \\\n",
       "1  Fox News contributor Lawrence Jones speaks to ...   \n",
       "\n",
       "                                         url  \\\n",
       "1  http://video.foxnews.com/v/6136724620001/   \n",
       "\n",
       "                                          urlToImage           publishedAt  \\\n",
       "1  https://cf-images.us-east-1.prod.boltdns.net/v...  2020-02-28T03:43:40Z   \n",
       "\n",
       "  content                                       TitleCleaned  \\\n",
       "1    None  [sanders, supporters, react, dnc, reportedly, ...   \n",
       "\n",
       "                                  DescriptionCleaned ContentCleaned  \\\n",
       "1  [fox, news, contributor, lawrence, jones, spea...             []   \n",
       "\n",
       "                                         uniqueWords  \n",
       "1  {give, leadership, sanders, lawrence, fox, nom...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get unique words for each column\n",
    "uniqueWords = []\n",
    "\n",
    "for i in range(len(MarchNews['ContentCleaned'])):\n",
    "    # create empty lsit each iteration\n",
    "    lst = []\n",
    "    # get all word in title, content, and description\n",
    "    [lst.append(words) for words in MarchNews['TitleCleaned'][i]]\n",
    "    [lst.append(words) for words in MarchNews['ContentCleaned'][i]]\n",
    "    [lst.append(words) for words in MarchNews['DescriptionCleaned'][i]]\n",
    "    #transform list into set of unique words\n",
    "    unique = set(lst)\n",
    "    #assign unique word set to each row or whole df\n",
    "    uniqueWords.append(unique)\n",
    "    \n",
    "# create new row in big DF with unique words\n",
    "MarchNews.insert(11,'uniqueWords', uniqueWords)\n",
    "MarchNews[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chief', 'white', 'tammy', 'south', 'campaign', 'reince', 'die', 'fox', 'biden', 'priebus', 'bruce', 'staff', 'carolina', 'primary', 'reaction', 'contributor', 'news', 'house', 'former'}\n"
     ]
    }
   ],
   "source": [
    "print(MarchNews['uniqueWords'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9287"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get unique words for whole dataset\n",
    "uniqueWords_all = []\n",
    "\n",
    "for i in range(len(MarchNews['ContentCleaned'])):\n",
    "    # get all words in title, content, and description\n",
    "    [uniqueWords_all.append(words) for words in MarchNews['TitleCleaned'][i]]\n",
    "    [uniqueWords_all.append(words) for words in MarchNews['ContentCleaned'][i]]\n",
    "    [uniqueWords_all.append(words) for words in MarchNews['DescriptionCleaned'][i]]\n",
    "uniqueWords_all = set(uniqueWords_all)\n",
    "\n",
    "len(uniqueWords_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>closed</th>\n",
       "      <th>prompt</th>\n",
       "      <th>artistas</th>\n",
       "      <th>stand</th>\n",
       "      <th>gad</th>\n",
       "      <th>vin</th>\n",
       "      <th>achondroplasia</th>\n",
       "      <th>tourism</th>\n",
       "      <th>seems</th>\n",
       "      <th>skepticism</th>\n",
       "      <th>...</th>\n",
       "      <th>analyzed</th>\n",
       "      <th>rests</th>\n",
       "      <th>populations</th>\n",
       "      <th>wildly</th>\n",
       "      <th>secured</th>\n",
       "      <th>televised</th>\n",
       "      <th>focused</th>\n",
       "      <th>marzio</th>\n",
       "      <th>日本新型コロナウイルス</th>\n",
       "      <th>interested</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 9287 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [closed, prompt, artistas, stand, gad, vin, achondroplasia, tourism, seems, skepticism, dewine, presidency, kept, ensure, reddit, foot, offs, semi, observing, como, note, neighbor, deeply, machine, tuesdayjudd, displaying, justice, half, rochelle, pervasive, olympians, experience, beating, tweeted, confident, launches, discordant, roles, venerable, joint, selfies, orders, waiving, michel, blames, analytica, comics, style, investor, muslims, used, ruhle, school, going, andrea, woke, banjo, hiatus, contribute, diagnostics, narratives, colin, delayspence, urine, parts, dept, groce, dickinson, tennessee, raiva, sealing, connected, tensionsmore, corporations, inquest, campaigning, outrage, doocy, lethality, solo, cuban, boot, hug, cbsn, competitive, washing, spotlight, slamming, office, bilingual, deflect, quarter, e, precinct, trage, whose, authorising, anunciou, driver, liberal, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 9287 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create empty df with unique words as col names (over 9000 cols)\n",
    "wordCountMatrix = pd.DataFrame(index=[], columns=uniqueWords_all)\n",
    "wordCountMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in wordCountMatrix (count of each word within each article)\n",
    "\n",
    "# iterate through whole dataset\n",
    "for row in range(len(MarchNews['ContentCleaned'])):\n",
    "    \n",
    "    # initialize empty count list\n",
    "    count_lst = []\n",
    "\n",
    "    # set unique words as column names\n",
    "    cols = MarchNews['uniqueWords'][row]\n",
    "\n",
    "    # iterate through column names\n",
    "    for i in cols:\n",
    "        # start count at 0, add counted words from each section \n",
    "        counter = 0\n",
    "        counter += MarchNews['TitleCleaned'][row].count(i)\n",
    "        counter += MarchNews['ContentCleaned'][row].count(i)\n",
    "        counter += MarchNews['DescriptionCleaned'][row].count(i)\n",
    "\n",
    "        # add final count of words to list\n",
    "        count_lst.append(counter)\n",
    "\n",
    "    # create df with this article'ss unique words as col names, and counts as a row\n",
    "    count_row_temp_df = pd.DataFrame([count_lst], columns=cols)\n",
    "\n",
    "    # append this df/row to wordCountMatrix \n",
    "    wordCountMatrix = wordCountMatrix.append(count_row_temp_df, ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaNs with 0\n",
    "wordCountMatrix = wordCountMatrix.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>closed</th>\n",
       "      <th>prompt</th>\n",
       "      <th>artistas</th>\n",
       "      <th>stand</th>\n",
       "      <th>gad</th>\n",
       "      <th>vin</th>\n",
       "      <th>achondroplasia</th>\n",
       "      <th>tourism</th>\n",
       "      <th>seems</th>\n",
       "      <th>skepticism</th>\n",
       "      <th>...</th>\n",
       "      <th>analyzed</th>\n",
       "      <th>rests</th>\n",
       "      <th>populations</th>\n",
       "      <th>wildly</th>\n",
       "      <th>secured</th>\n",
       "      <th>televised</th>\n",
       "      <th>focused</th>\n",
       "      <th>marzio</th>\n",
       "      <th>日本新型コロナウイルス</th>\n",
       "      <th>interested</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1779</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1780</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1781</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1782</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1783</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1784 rows × 9287 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      closed  prompt  artistas  stand  gad  vin  achondroplasia  tourism  \\\n",
       "0          0       0         0      0    0    0               0        0   \n",
       "1          0       0         0      0    0    0               0        0   \n",
       "2          0       0         0      0    0    0               0        0   \n",
       "3          0       0         0      0    0    0               0        0   \n",
       "4          0       0         0      0    0    0               0        0   \n",
       "...      ...     ...       ...    ...  ...  ...             ...      ...   \n",
       "1779       0       0         0      0    0    0               0        0   \n",
       "1780       0       0         0      0    0    0               0        0   \n",
       "1781       0       0         0      0    0    0               0        0   \n",
       "1782       0       0         0      0    0    0               0        0   \n",
       "1783       0       0         0      0    0    0               0        0   \n",
       "\n",
       "      seems  skepticism  ...  analyzed  rests  populations  wildly  secured  \\\n",
       "0         0           0  ...         0      0            0       0        0   \n",
       "1         0           0  ...         0      0            0       0        0   \n",
       "2         0           0  ...         0      0            0       0        0   \n",
       "3         0           0  ...         0      0            0       0        0   \n",
       "4         0           0  ...         0      0            0       0        0   \n",
       "...     ...         ...  ...       ...    ...          ...     ...      ...   \n",
       "1779      0           0  ...         0      0            0       0        0   \n",
       "1780      0           0  ...         0      0            0       0        0   \n",
       "1781      0           0  ...         0      0            0       0        0   \n",
       "1782      0           0  ...         0      0            0       0        0   \n",
       "1783      0           0  ...         0      0            0       0        0   \n",
       "\n",
       "      televised  focused  marzio  日本新型コロナウイルス  interested  \n",
       "0             0        0       0            0           0  \n",
       "1             0        0       0            0           0  \n",
       "2             0        0       0            0           0  \n",
       "3             0        0       0            0           0  \n",
       "4             0        0       0            0           0  \n",
       "...         ...      ...     ...          ...         ...  \n",
       "1779          0        0       0            0           0  \n",
       "1780          0        0       0            0           0  \n",
       "1781          0        0       0            0           0  \n",
       "1782          0        0       0            0           0  \n",
       "1783          0        0       0            0           0  \n",
       "\n",
       "[1784 rows x 9287 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCountMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       0\n",
       "2       1\n",
       "3       5\n",
       "4       2\n",
       "       ..\n",
       "1779    0\n",
       "1780    0\n",
       "1781    0\n",
       "1782    0\n",
       "1783    0\n",
       "Name: biden, Length: 1784, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCountMatrix['biden']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The unique words stuff after this section is likely not going to be used, but could be a good point of reference for future code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vCL6NCrbaakT"
   },
   "source": [
    "# Get Unique Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uAJw3DmQaakT"
   },
   "source": [
    "#### In order to use a machine learning algrorithm, we first needed to convert our data into a useable format since we can't train an agorithm on words alone. We originally tried to match and count words in an article to a set of predefined, politically loaded words and terms. These polarizing terms included \"death tax\", \"estate tax\", \"pro life\", \"pro choice\", \"gun safety\", \"gun control\", and more. However, we found that matching these terms to news from the last month is not very effective since most of the coverage has been on COVID-19 and the democratic primary. \n",
    "\n",
    "#### We plan on reading through some of the articles and creating a new matrix of words and terms to match to/count in earch article. Before that is done, we first need to prep the data. To do this, we first extracted a list of unique words from each news article. Once we have a new matrix of political words and terms, we can iterate through each list of unique words to see if any of our words of interest appear in that article. If a word appears, we can then iterate through the words in that article to get a word count. The final result of this process would be a matrix where words/phrases make up the columns, and each row represents an article, and the cells would be populated with numbers representing the count of that columns word or phrase in the article. This matrix could then be fed into an algorithm such as SVM, averaged Perceptron, or Logistic Regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yT2jC112aakU"
   },
   "outputs": [],
   "source": [
    "# get unique words for each column\n",
    "uniqueWords = []\n",
    "\n",
    "for i in range(len(MarchNews['ContentCleaned'])):\n",
    "    # create empty lsit each iteration\n",
    "    lst = []\n",
    "    # get all word in title, content, and description\n",
    "    [lst.append(words) for words in MarchNews['TitleCleaned'][i]]\n",
    "    [lst.append(words) for words in MarchNews['ContentCleaned'][i]]\n",
    "    [lst.append(words) for words in MarchNews['DescriptionCleaned'][i]]\n",
    "    #transform list into set of unique words\n",
    "    unique = set(lst)\n",
    "    #assign unique word set to each row or whole df\n",
    "    uniqueWords.append(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-5sO79puaakW",
    "outputId": "c8d211f3-6079-4733-d27d-e58956075ad7"
   },
   "outputs": [],
   "source": [
    "# create new row in big DF with unique words\n",
    "MarchNews.insert(11,'uniqueWords', uniqueWords)\n",
    "MarchNews[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1_sB3sZoaakZ",
    "outputId": "2aa2aed7-0570-4d6f-ed0d-7b3cd47e176e"
   },
   "outputs": [],
   "source": [
    "print(MarchNews['uniqueWords'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gV8AIg-daakc"
   },
   "source": [
    "#### We are also considering comparing the unique words between news sources as a way of training an algorithm. The code below extracts all of the unique words used by each news source.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K9sbvR2Raakd"
   },
   "outputs": [],
   "source": [
    "# group df by source\n",
    "# index numbers--> 0 = Breitbart, 1 = Buzzfeed, 2 = CBS, 3 = Fox, 4 = MSNBC, \n",
    "#                  5 = National-Review, 6 = Reuters, 7 = The Hill, 8 = Vice News \n",
    "uniqueWords_allSources = MarchNews.groupby('source')['uniqueWords'].apply(list)\n",
    "\n",
    "# right\n",
    "breitbart_words = uniqueWords_allSources.iloc[[0]]\n",
    "fox_words = uniqueWords_allSources.iloc[[3]]\n",
    "natReview_words = uniqueWords_allSources.iloc[[5]]\n",
    "\n",
    "# middle\n",
    "cbs_words = uniqueWords_allSources.iloc[[2]]\n",
    "reuters_words = uniqueWords_allSources.iloc[[6]]\n",
    "hill_words = uniqueWords_allSources.iloc[[7]]\n",
    "\n",
    "# left\n",
    "buzzfeed_words = uniqueWords_allSources.iloc[[1]]\n",
    "msnbc_words = uniqueWords_allSources.iloc[[4]]\n",
    "vice_words = uniqueWords_allSources.iloc[[8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8kD_MUYBaakh",
    "outputId": "8eb4edd0-00fe-4ba7-b548-f9c127bc6b21"
   },
   "outputs": [],
   "source": [
    "# function that creates set of unique words for each source\n",
    "\n",
    "def unique_by_source(series):\n",
    "    lst =[]\n",
    "    for i in series[0]:\n",
    "        lst.extend(i)\n",
    "    new_set = set(lst)\n",
    "    return new_set\n",
    "\n",
    "# right\n",
    "unique_breitbart_words = unique_by_source(breitbart_words)\n",
    "unique_fox_words = unique_by_source(fox_words)\n",
    "unique_natReview_words = unique_by_source(natReview_words)\n",
    "\n",
    "# middle\n",
    "unique_cbs_words = unique_by_source(cbs_words)\n",
    "unique_reuters_words = unique_by_source(reuters_words)\n",
    "unique_hill_words = unique_by_source(hill_words)\n",
    "\n",
    "# left\n",
    "unique_buzzfeed_words = unique_by_source(buzzfeed_words) \n",
    "unique_msnbc_words = unique_by_source(msnbc_words)\n",
    "unique_vice_words = unique_by_source(vice_words)\n",
    "\n",
    "\n",
    "Left = set(list(unique_buzzfeed_words) + list(unique_msnbc_words) + list(unique_vice_words))\n",
    "Middle = set(list(unique_cbs_words) + list(unique_reuters_words) + list(unique_hill_words))\n",
    "Right = set(list(unique_breitbart_words) + list(unique_fox_words) + list(unique_natReview_words))\n",
    "\n",
    "\n",
    "UniquelyLeft = [w for w in Left if w not in Middle and w not in Right]\n",
    "\n",
    "UniquelyMiddle = [w for w in Middle if w not in Left and w not in Right]\n",
    "\n",
    "UniquelyRight = [w for w in Right if w not in Left and w not in Middle]\n",
    "\n",
    "print(len(UniquelyLeft))\n",
    "print(len(UniquelyRight))\n",
    "print(len(UniquelyMiddle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'sanders' in UniquelyMiddle"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DetectingMediaBias.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
